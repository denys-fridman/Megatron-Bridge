#!/bin/bash
#
# Generated by NeMo Run
# Run with: sbatch --requeue --parsable
#

# Parameters
#SBATCH --account=coreai_mlperf_training
#SBATCH --exclusive
#SBATCH --job-name=coreai_mlperf_training-training.deepseek_v3_conversion
#SBATCH --mem=0
#SBATCH --nodes=32
#SBATCH --ntasks-per-node=1
#SBATCH --open-mode=append
#SBATCH --output=/lustre/fsw/coreai_mlperf_training/users/dfridman/checkpoints/slurm_logs/slurm_%j.out
#SBATCH --partition=batch
#SBATCH --time=00:30:00

set -evx

set +e

nodes=( $( scontrol show hostnames $SLURM_JOB_NODELIST ) )
nodes_array=($nodes)
head_node=${nodes_array[0]}
head_node_ip=$(srun --nodes=1 --ntasks=1 -w "$head_node" hostname --ip-address)

export HF_HOME=/checkpoints/hf
export NCCL_MNNVL_ENABLE=0

srun --container-mounts /lustre/fsw/coreai_mlperf_training/users/dfridman/checkpoints:/checkpoints,/lustre/fsw/coreai_mlperf_training/users/dfridman/Megatron-Bridge/examples/conversion/hf_megatron_roundtrip_multi_gpu.py:/workspace/Megatron-Bridge/examples/conversion/hf_megatron_roundtrip_multi_gpu.py,/lustre/fsw/coreai_mlperf_training/users/dfridman/Megatron-Bridge/src/megatron/bridge/models/deepseek:/workspace/Megatron-Bridge/src/megatron/bridge/models/deepseek \
     --container-image gitlab-master.nvidia.com/dl/mlperf/optimized:deepseekv3_671b.pytorch.41325967 \
     --no-container-mount-home \
     torchrun \
       --nnodes 32 \
       --nproc_per_node 8 \
       --rdzv_id $RANDOM \
       --rdzv_backend c10d \
       --rdzv_endpoint $head_node_ip:29500 \
       /workspace/Megatron-Bridge/examples/conversion/hf_megatron_roundtrip_multi_gpu.py \
       --hf-model-id deepseek-ai/DeepSeek-V3-Base \
       --tp 1 --pp 4 --vp 4 --ep 64 \
       --megatron-save-path /checkpoints/megatron/DeepSeek-V3-Base-bf16 \
       --trust-remote-code
